{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "import torchvision.models as models\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Union, Optional, List\n",
    "\n",
    "from catalyst.utils import get_device\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_name = \"resnet34_best.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_string = '''\n",
    "dataset:\n",
    "  test:\n",
    "    affine: False\n",
    "    morphology: False\n",
    "\n",
    "data:\n",
    "  test_df_path: ../input/bengaliai-cv19/train.csv\n",
    "  test_parquet_path:\n",
    "    - ../input/bengaliai-cv19/test_image_data_0.parquet\n",
    "    - ../input/bengaliai-cv19/test_image_data_1.parquet\n",
    "    - ../input/bengaliai-cv19/test_image_data_2.parquet\n",
    "    - ../input/bengaliai-cv19/test_image_data_3.parquet\n",
    "  sample_submission_path: ../input/bengaliai-cv19/sample_submission.csv\n",
    "\n",
    "bin:\n",
    "  - ../input/bengali-resnet34-init/fold0.pth\n",
    "  - ../input/bengali-resnet34-init/fold1.pth\n",
    "  - ../input/bengali-resnet34-init/fold2.pth\n",
    "  - ../input/bengali-resnet34-init/fold3.pth\n",
    "  - ../input/bengali-resnet34-init/fold4.pth\n",
    "\n",
    "model:\n",
    "  model_name: resnet34\n",
    "  pretrained: True\n",
    "  num_classes: 186\n",
    "  head: custom\n",
    "  in_channels: 3\n",
    "\n",
    "test:\n",
    "  batch_size: 128\n",
    "\n",
    "transforms:\n",
    "  test:\n",
    "    HorizontalFlip: False\n",
    "    VerticalFlip: False\n",
    "    Noise: False\n",
    "    Contrast: False\n",
    "    Rotate: False\n",
    "    RandomScale: False\n",
    "    Cutout:\n",
    "      num_holes: 0\n",
    "  mean: [0.485, 0.456, 0.406]\n",
    "  std: [0.229, 0.224, 0.225]\n",
    "\n",
    "num_workers: 2\n",
    "seed: 1213\n",
    "img_size: 128\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(yaml.load(conf_string, Loader=yaml.SafeLoader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and utilities preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(config: dict, phase: str = \"train\"):\n",
    "    assert phase in [\"train\", \"valid\", \"test\"]\n",
    "    if phase == \"train\":\n",
    "        cfg = config[\"transforms\"][\"train\"]\n",
    "    elif phase == \"valid\":\n",
    "        cfg = config[\"transforms\"][\"val\"]\n",
    "    elif phase == \"test\":\n",
    "        cfg = config[\"transforms\"][\"test\"]\n",
    "    list_transforms = []\n",
    "    if cfg[\"HorizontalFlip\"]:\n",
    "        list_transforms.append(A.HorizontalFrip())\n",
    "    if cfg[\"VerticalFlip\"]:\n",
    "        list_transforms.append(A.VerticalFlip())\n",
    "    if cfg[\"Rotate\"]:\n",
    "        list_transforms.append(A.Rotate(limit=15))\n",
    "    if cfg[\"RandomScale\"]:\n",
    "        list_transforms.append(A.RandomScale())\n",
    "    if cfg[\"Noise\"]:\n",
    "        list_transforms.append(\n",
    "            A.OneOf(\n",
    "                [A.GaussNoise(), A.IAAAdditiveGaussianNoise()], p=0.5))\n",
    "    if cfg[\"Contrast\"]:\n",
    "        list_transforms.append(\n",
    "            A.OneOf(\n",
    "                [A.RandomContrast(0.5),\n",
    "                 A.RandomGamma(),\n",
    "                 A.RandomBrightness()],\n",
    "                p=0.5))\n",
    "    if cfg[\"Cutout\"][\"num_holes\"] > 0:\n",
    "        list_transforms.append(A.Cutout(**config.Cutout))\n",
    "\n",
    "    list_transforms.append(\n",
    "        A.Normalize(\n",
    "            mean=config[\"transforms\"][\"mean\"], std=config[\"transforms\"][\"std\"], p=1))\n",
    "\n",
    "    return A.Compose(list_transforms, p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config[\"data\"][\"test_df_path\"])\n",
    "transforms_dict = {\"test\": get_transforms(config, \"test\")}\n",
    "cls_levels = {\n",
    "    \"grapheme\": 168,\n",
    "    \"vowel\": 11,\n",
    "    \"consonant\": 7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTestDataset(torchdata.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transforms, size: Tuple[int, int]):\n",
    "        self.images = df.iloc[:, 1:].values.reshape(-1, 137, 236)\n",
    "        self.size = size\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if image.ndim == 2:\n",
    "            image = np.moveaxis(np.stack([image, image, image]), 0, -1)\n",
    "        image = cv2.resize(image, self.size)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)[\"image\"]\n",
    "        image = cv2.resize(image, self.size)\n",
    "        if image.shape[2] == 3:\n",
    "            image = np.moveaxis(image, -1, 0)\n",
    "        return image\n",
    "    \n",
    "    \n",
    "def get_base_test_loader(df: pd.DataFrame,\n",
    "                         size: Tuple[int, int] = (128, 128),\n",
    "                         batch_size=256,\n",
    "                         num_workers=2,\n",
    "                         transforms=None):\n",
    "    dataset = BaseTestDataset(df, transforms, size)\n",
    "    return torchdata.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gem(x: torch.Tensor, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p),\n",
    "                        (x.size(-2), x.size(-1))).pow(1. / p)\n",
    "\n",
    "\n",
    "def mish(input):\n",
    "    '''\n",
    "    Applies the mish function element-wise:\n",
    "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    See additional documentation for mish class.\n",
    "    '''\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    '''\n",
    "    Applies the mish function element-wise:\n",
    "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    Examples:\n",
    "        >>> m = Mish()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return mish(input)\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(\n",
    "            self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 num_classes: int,\n",
    "                 pretrained=False,\n",
    "                 head=\"linear\",\n",
    "                 in_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.base = getattr(models, model_name)(pretrained=pretrained)\n",
    "        self.head = head\n",
    "        assert in_channels in [1, 3]\n",
    "        assert head in [\"linear\", \"custom\"]\n",
    "        if in_channels == 1:\n",
    "            if pretrained:\n",
    "                weight = self.base.conv1.weight\n",
    "                self.base.conv1 = nn.Conv2d(\n",
    "                    1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "                self.base.conv1.weight = nn.Parameter(\n",
    "                    data=torch.mean(weight, dim=1, keepdim=True),\n",
    "                    requires_grad=True)\n",
    "            else:\n",
    "                self.base.conv1 = nn.Conv2d(\n",
    "                    1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        if head == \"linear\":\n",
    "            n_in_features = self.base.fc.in_features\n",
    "            self.base.fc = nn.Linear(n_in_features, self.num_classes)\n",
    "        elif head == \"custom\":\n",
    "            n_in_features = self.base.fc.in_features\n",
    "            arch = list(self.base.children())\n",
    "            for _ in range(2):\n",
    "                arch.pop()\n",
    "            self.base = nn.Sequential(*arch)\n",
    "            self.grapheme_head = nn.Sequential(\n",
    "                Mish(), nn.Conv2d(n_in_features, 512, kernel_size=3),\n",
    "                nn.BatchNorm2d(512), GeM(), nn.Linear(512, 168))\n",
    "            self.vowel_head = nn.Sequential(\n",
    "                Mish(), nn.Conv2d(n_in_features, 512, kernel_size=3),\n",
    "                nn.BatchNorm2d(512), GeM(), nn.Linear(512, 11))\n",
    "            self.consonant_head = nn.Sequential(\n",
    "                Mish(), nn.Conv2d(n_in_features, 512, kernel_size=3),\n",
    "                nn.BatchNorm2d(512), GeM(), nn.Linear(512, 7))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.head == \"linear\":\n",
    "            return self.base(x)\n",
    "        elif self.head == \"custom\":\n",
    "            x = self.base(x)\n",
    "            grapheme = self.grapheme_head(x)\n",
    "            vowel = self.vowel_head(x)\n",
    "            consonant = self.consonant_head(x)\n",
    "            return torch.cat([grapheme, vowel, consonant], dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_model(config: dict):\n",
    "    params = config[\"model\"]\n",
    "    if \"resnet\" in params[\"model_name\"]:\n",
    "        return Resnet(**params)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config: dict, bin_path: Union[str, Path]):\n",
    "    model = get_model(config)\n",
    "    state_dict = torch.load(bin_path, map_location=get_device())\n",
    "    if \"model_state_dict\" in state_dict.keys():\n",
    "        model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(model: nn.Module,\n",
    "                   loader: torchdata.DataLoader,\n",
    "                   cls_levels: dict,\n",
    "                   loss_fn: Optional[nn.Module] = None,\n",
    "                   requires_soft=False):\n",
    "    n_grapheme = cls_levels[\"grapheme\"]\n",
    "    n_vowel = cls_levels[\"vowel\"]\n",
    "    n_consonant = cls_levels[\"consonant\"]\n",
    "\n",
    "    dataset_length = len(loader.dataset)\n",
    "    prediction = np.zeros((dataset_length, 3), dtype=np.uint8)\n",
    "    if requires_soft:\n",
    "        soft_prediction = np.zeros(\n",
    "            (dataset_length, n_grapheme + n_vowel, n_consonant),\n",
    "            dtype=np.uint8)\n",
    "\n",
    "    batch_size = loader.batch_size\n",
    "    device = get_device()\n",
    "\n",
    "    avg_loss = 0.\n",
    "    model.eval()\n",
    "\n",
    "    targets: Optional[torch.Tensor] = None\n",
    "\n",
    "    for i, batch in enumerate(progress_bar(loader, leave=False)):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(batch, dict):\n",
    "                images = batch[\"images\"].to(device)\n",
    "                targets = batch[\"targets\"].to(device)\n",
    "            else:\n",
    "                images = batch.to(device)\n",
    "                targets = None\n",
    "            pred = model(images).detach()\n",
    "            if loss_fn is not None and targets is not None:\n",
    "                avg_loss += loss_fn(\n",
    "                    pred, batch[\"targets\"].to(device)).item() / len(loader)\n",
    "            head = 0\n",
    "            tail = n_grapheme\n",
    "            pred_grapheme = torch.argmax(\n",
    "                pred[:, head:tail], dim=1).cpu().numpy()\n",
    "\n",
    "            head = tail\n",
    "            tail = head + n_vowel\n",
    "            pred_vowel = torch.argmax(pred[:, head:tail], dim=1).cpu().numpy()\n",
    "\n",
    "            head = tail\n",
    "            tail = head + n_consonant\n",
    "            pred_consonant = torch.argmax(\n",
    "                pred[:, head:tail], dim=1).cpu().numpy()\n",
    "\n",
    "            prediction[i * batch_size:(i + 1) * batch_size, 0] = pred_grapheme\n",
    "            prediction[i * batch_size:(i + 1) * batch_size, 1] = pred_vowel\n",
    "            prediction[i * batch_size:(i + 1) * batch_size, 2] = pred_consonant\n",
    "\n",
    "            if requires_soft:\n",
    "                head = 0\n",
    "                tail = n_grapheme\n",
    "                soft_prediction[i * batch_size:(i + 1) *\n",
    "                                batch_size, head:tail] = F.softmax(\n",
    "                                    pred[:, head:tail], dim=1).cpu().numpy()\n",
    "\n",
    "                head = tail\n",
    "                tail = head + n_vowel\n",
    "                soft_prediction[i * batch_size:(i + 1) *\n",
    "                                batch_size, head:tail] = F.softmax(\n",
    "                                    pred[:, head:tail], dim=1).cpu().numpy()\n",
    "\n",
    "                head = tail\n",
    "                tail = head + n_consonant\n",
    "                soft_prediction[i * batch_size:(i + 1) *\n",
    "                                batch_size, head:tail] = F.softmax(\n",
    "                                    pred[:, head:tail], dim=1).cpu().numpy()\n",
    "\n",
    "    return_dict = {\"prediction\": prediction, \"loss\": avg_loss}\n",
    "    if requires_soft:\n",
    "        return_dict[\"soft_prediction\"] = soft_prediction\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\n",
    "target_grapheme = []\n",
    "target_vowel = []\n",
    "target_consonant = []\n",
    "row_id = [] # row_id place holder\n",
    "\n",
    "n_grapheme = cls_levels[\"grapheme\"]\n",
    "n_vowel = cls_levels[\"vowel\"]\n",
    "n_consonant = cls_levels[\"consonant\"]\n",
    "\n",
    "parquets = config[\"data\"][\"test_parquet_path\"]\n",
    "for path in parquets:\n",
    "    df = pd.read_parquet(path)\n",
    "    row_id.extend(df[\"image_id\"].values)\n",
    "    \n",
    "    loader = get_base_test_loader(\n",
    "        df,\n",
    "        size=(config[\"img_size\"], config[\"img_size\"]),\n",
    "        batch_size=config[\"test\"][\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        transforms=transforms_dict[\"test\"])\n",
    "    binaries = config[\"bin\"]\n",
    "    result_array = np.zeros((len(df), 186))\n",
    "    for binary in binaries:\n",
    "        model = load_model(config, binary)\n",
    "        prediction = inference_loop(\n",
    "            model,\n",
    "            loader,\n",
    "            cls_levels,\n",
    "            loss_fn=None,\n",
    "            requires_soft=True)\n",
    "        result_array += prediction[\"soft_prediction\"] / len(binaries)\n",
    "    \n",
    "    head = 0\n",
    "    tail = n_grapheme\n",
    "    grapheme_preds = np.argmax(result_array[:, head:tail], axis=1)\n",
    "    \n",
    "    head = tail\n",
    "    tail = n_vowel\n",
    "    vowel_preds = np.argmax(result_array[:, head:tail], axis=1)\n",
    "    \n",
    "    head = tail\n",
    "    tail = n_consonant\n",
    "    consonant_preds = np.argmax(result_array[:, head:tail], axis=1)\n",
    "    \n",
    "    target_grapheme.extend(grapheme_preds)\n",
    "    target_vowel.extend(vowel_preds)\n",
    "    target_consonant.extend(consonant_preds)\n",
    "    \n",
    "prediction_df = pd.DataFrame({\n",
    "    \"image_id\": row_id,\n",
    "    \"grapheme_root\": target_grapheme,\n",
    "    \"vowel_diacritic\": target_vowel,\n",
    "    \"consonant_diacritic\": target_consonant\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "result = []\n",
    "\n",
    "for i, row in prediction_df:\n",
    "    for target_name in components:\n",
    "        name.append(row.image_id + \"_\" + target_name)\n",
    "        result.append(row[target_name])\n",
    "        \n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": name,\n",
    "    \"target\": result\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
